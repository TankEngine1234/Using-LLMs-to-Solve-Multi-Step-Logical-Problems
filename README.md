# Multi-Step Logical Problem Analysis with LLMs

This repository contains R code and related resources for evaluating the problem-solving capabilities of Large Language Models (LLMs) on multi-step logical problems. The project involves analyzing performance metrics such as accuracy, logical consistency, efficiency, and error rate across various LLMs, including ChatGPT-4, Gemini, Llama, Claude, and Grok.

## Key Features

- **Evaluation Metrics**: Implementation of metrics to assess LLM reasoning performance.
- **Dataset Creation**: Scripts for custom multi-step logic puzzles (e.g., Zebra Puzzle, Tower of Hanoi).
- **Statistical Analysis**: Analysis and visualization of LLM performance trends.
- **Model Comparison**: Comparative results across multiple LLMs.

## Getting Started

### Prerequisites

- R (latest version)
- RStudio (optional but recommended)
- Required R packages:
  - `ggplot2`
  - `dplyr`
  - `tidyverse`

Install these packages using:
```R
install.packages(c("ggplot2", "dplyr", "tidyverse"))
```

### Installation

1. Clone the repository:
```bash
git clone https://github.com/your-username/multi-step-logic-llms.git
```
2. Open the R project file in RStudio.
3. Run the provided scripts to analyze the dataset and generate visualizations.

## Usage

- **Dataset Creation**: Use `dataset_creation.R` to generate and format custom logic puzzles.
- **Model Evaluation**: Use `evaluation_metrics.R` to calculate performance metrics for LLMs.
- **Visualization**: Run `performance_analysis.R` to create performance trend graphs.

## Contributing

Contributions are welcome! To contribute:
1. Fork the repository.
2. Create a new branch for your feature or bug fix.
3. Submit a pull request with a detailed description of your changes.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Contact

For questions or collaboration opportunities, please contact:
- **Name**: Nihal Thomas
- **Email**: nihalthomas12@gmail.com
